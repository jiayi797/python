Python 3.3.2 (v3.3.2:d047928ae3f6, May 16 2013, 00:03:43) [MSC v.1600 32 bit (Intel)] on win32
Type "copyright", "credits" or "license()" for more information.
>>> # -*- coding: utf-8 -*-  
#---------------------------------------  
#   程序：北京邮电大学爬虫  
#   版本：0.1  
#   作者：jiayi797  
#   日期：2013-12-08 
#   语言：Python 3.3  
#   操作：输入学号和密码  
#   功能：输出成绩的加权平均值也就是绩点   
#---------------------------------------  
  
import urllib
#-----------------------------------------------------------------------------------------------------
#   import 的实质
#   import的过程实际上包括两个主要部分,
#   1) 类似于shell 的 "source" 命令, 具体说即相当于将被import的module(即python文件)在当前环境下执行一遍.
#   当多次import同一个module时, python会保证只执行一次. 参考Example1
#   2) import的第二个部分是使被import的成员(变量名, 函数名, 类....)可见.
#   为保证不发生命名冲突, 需要以 module.name 的方式访问导入的成员
#   URI的概念和举例
#   简单的来讲，URL就是在浏览器端输入的    www.baidu.com    这个字符串。
#   在理解URL之前，首先要理解URI的概念。
#   Web上每种可用的资源，如 HTML文档、图像、视频片段、程序等都由一个通用资源标志符(Universal Resource Identifier， URI)进行定位。 
#   URI通常由三部分组成：
#   ①访问资源的命名机制；
#   ②存放资源的主机名；
#   ③资源自身 的名称，由路径表示。
#   如下面的URI：http://www.why.com.cn/myhtml/html1223/
#   我们可以这样解释它：
#   ①这是一个可以通过HTTP协议访问的资源，
#   ②位于主机 www.webmonkey.com.cn上，
#   ③通过路径“/html/html40”访问。 '''
#-----------------------------------------------------------------------------------------------------

#   URL是URI的一个子集。它是Uniform Resource Locator的缩写，译为“统一资源定位 符”。
#   通俗地说，URL是Internet上描述信息资源的字符串，主要用在各种WWW客户程序和服务器程序上。
#   采用URL可以用一种统一的格式来描述各种信息资源，包括文件、服务器的地址和目录等。
#   URL的格式由三部分组成： 
#   ①第一部分是协议(或称为服务方式)。
#   ②第二部分是存有该资源的主机IP地址(有时也包括端口号)。
#   ③第三部分是主机资源的具体地址，如目录和文件名等。
#   第一部分和第二部分用“://”符号隔开，
#   第二部分和第三部分用“/”符号隔开。
#   第一部分和第二部分是不可缺少的，第三部分有时可以省略。 
#-----------------------------------------------------------------------------------------------------
import urllib2  

#-----------------------------------------------------------------------------------------------------
#urllib2的使用细节:

#1.Proxy 的设置
#urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy。
#如果想在程序中明确控制 Proxy 而不受环境变量的影响，可以使用代理。
#这里要注意的一个细节，使用 urllib2.install_opener() 会设置 urllib2 的全局 opener 。
#这样后面的使用会很方便，但不能做更细致的控制，比如想在程序中使用两个不同的 Proxy 设置等。
#比较好的做法是不使用 install_opener 去更改全局的设置，而只是直接调用 opener 的 open 方法代替全局的 urlopen 方法。

#2.Timeout 设置
#在老版 Python 中（Python2.6前），urllib2 的 API 并没有暴露 Timeout 的设置，要设置 Timeout 值，
#只能更改 Socket 的全局 Timeout 值。在 Python 2.6 以后，超时可以通过 urllib2.urlopen() 的 timeout 参数直接设置。

#3.在 HTTP Request 中加入特定的 Header
#要加入 header，需要使用 Request 对象：
#对有些 header 要特别留意，服务器会针对这些 header 做检查
#User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求
#Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。常见的取值有：
#application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用
#application/json ： 在 JSON RPC 调用时使用
#application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用
#在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务

#4.Redirect
#urllib2 默认情况下会针对 HTTP 3XX 返回码自动进行 redirect 动作，无需人工配置。
#要检测是否发生了 redirect 动作，只要检查一下 Response 的 URL 和 Request 的 URL 是否一致就可以了。
#如果不想自动 redirect，除了使用更低层次的 httplib 库之外，还可以自定义HTTPRedirectHandler 类。

#5.Cookie
#urllib2 对 Cookie 的处理也是自动的。如果需要得到某个 Cookie 项的值，可以这么做：

#6.使用 HTTP 的 PUT 和 DELETE 方法
#urllib2 只支持 HTTP 的 GET 和 POST 方法，如果要使用 HTTP PUT 和 DELETE ，只能使用比较低层的 httplib 库。
#虽然如此，我们还是能通过下面的方式，使 urllib2 能够发出 PUT 或DELETE 的请求,得到 HTTP 的返回码
#对于 200 OK 来说，只要使用 urlopen 返回的 response 对象的 getcode() 方法就可以得到 HTTP 的返回码。
#但对其它返回码来说，urlopen 会抛出异常。这时候，就要检查异常对象的 code 属性了.

#8.Debug Log
#使用 urllib2 时，可以通过下面的方法把 debug Log 打开，这样收发包的内容就会在屏幕上打印出来，方便调试，
#有时可以省去抓包的工作

9.表单的处理
#登录必要填表，表单怎么填？
#首先利用工具截取所要填表的内容。
#比如我一般用firefox+httpfox插件来看看自己到底发送了些什么包。
#以verycd为例，先找到自己发的POST请求，以及POST表单项。
#可以看到verycd的话需要填username,password,continueURI,fk,login_submit这几项，其中fk是随机生成的
#（其实不太随机，看上去像是把epoch时间经过简单的编码生成的），需要从网页获取，也就是说得先访问一次网页，
#用正则表达式等工具截取返回数据中的fk项。continueURI顾名思义可以随便写，login_submit是固定的，
#这从源码可以看出。还有username，password那就很显然了.

#10.伪装成浏览器访问
#某些网站反感爬虫的到访，于是对爬虫一律拒绝请求
#这时候我们需要伪装成浏览器，这可以通过修改http包中的header来实现

#11.对付"反盗链"
#某些站点有所谓的反盗链设置，其实说穿了很简单，
#就是检查你发送请求的header里面，referer站点是不是他自己，
#所以我们只需要像把headers的referer改成该网站即可.
#headers是一个dict数据结构，你可以放入任何想要的header，来做一些伪装。
#例如，有些网站喜欢读取header中的X-Forwarded-For来看看人家的真实IP，可以直接把X-Forwarde-For改了。
#-----------------------------------------------------------------------------------------------------
import cookielib  
import re  
  
class SDU_Spider:    
    # 申明相关的属性    
    def __init__(self):      
        self.loginUrl = 'http://211.68.70.198:8080/pls/wwwbks/bks_login2.login'   # 登录的url  
        self.resultUrl = 'http://211.68.70.198:8080/pls/wwwbks/bkscjcx.curscopre' # 显示成绩的url  
        self.cookieJar = cookielib.CookieJar()                                      # 初始化一个CookieJar来处理Cookie的信息  
        self.postdata=urllib.urlencode({'stuid':'2012210031','pwd':'老师我怎么能告诉你密码'})     # POST的数据  
        self.weights = []   #存储权重，也就是学分  
        self.points = []    #存储分数，也就是成绩  
        self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(self.cookieJar))
#-----------------------------------------------------------------------------------------------------
两个重要概念：Openers和Handlers。
1.Openers：
当你获取一个URL你使用一个opener(一个urllib2.OpenerDirector的实例)。
正常情况下，我们使用默认opener：通过urlopen。
但你能够创建个性的openers。
2.Handles：
Openers使用处理器handlers，所有的“繁重”工作由handlers处理。
每个handlers知道如何通过特定协议打开URLs，或者如何处理URL打开时的各个方面。
例如HTTP重定向或者HTTP cookies。

如果你希望用特定处理器获取URLs你会想创建一个openers，例如获取一个能处理cookie的opener，或者获取一个不重定向的opener。

要创建一个 opener，可以实例化一个OpenerDirector，
然后调用.add_handler(some_handler_instance)。
同样，可以使用build_opener，这是一个更加方便的函数，用来创建opener对象，他只需要一次函数调用。
build_opener默认添加几个处理器，但提供快捷的方法来添加或更新默认处理器。
其他的处理器handlers你或许会希望处理代理，验证，和其他常用但有点特殊的情况。

install_opener 用来创建（全局）默认opener。这个表示调用urlopen将使用你安装的opener。
Opener对象有一个open方法。
该方法可以像urlopen函数那样直接用来获取urls：通常不必调用install_opener，除了为了方便。
#-----------------------------------------------------------------------------------------------------  
    def sdu_init(self):  
        # 初始化链接并且获取cookie  
        myRequest = urllib2.Request(url = self.loginUrl,data = self.postdata)   # 自定义一个请求  
        result = self.opener.open(myRequest)            # 访问登录页面，获取到必须的cookie的值  
        result = self.opener.open(self.resultUrl)       # 访问成绩页面，获得成绩的数据  
        # 打印返回的内容  
        # print result.read()  
        self.deal_data(result.read().decode('gbk'))  
        self.print_data(self.weights);  
        self.print_data(self.points);  
#-----------------------------------------------------------------------------------------------------
#   正则表达式是用于处理字符串的强大工具，它并不是Python的一部分。
#   其他编程语言中也有正则表达式的概念，区别只在于不同的编程语言实现支持的语法数量不同。
#   它拥有自己独特的语法以及一个独立的处理引擎，在提供了正则表达式的语言里，正则表达式的语法都是一样的.
#   正则表达式的大致匹配过程是：
#   1.依次拿出表达式和文本中的字符比较，
#   2.如果每一个字符都能匹配，则匹配成功；一旦有匹配不成功的字符则匹配失败。
#   3.如果表达式中有量词或边界，这个过程会稍微有一些不同。
#   Python通过re模块提供对正则表达式的支持。
#   使用re的一般步骤是：
#   Step1：先将正则表达式的字符串形式编译为Pattern实例。
#   Step2：然后使用Pattern实例处理文本并获得匹配结果（一个Match实例）。
#   Step3：最后使用Match实例获得信息，进行其他的操作。
#-----------------------------------------------------------------------------------------------------
    # 将内容从页面代码中抠出来    
    def deal_data(self,myPage):    
        myItems = re.findall('<TR>.*?<p.*?<p.*?<p.*?<p.*?<p.*?>(.*?)</p>.*?<p.*?<p.*?>(.*?)</p>.*?</TR>',myPage,re.S)     #获取到学分  
        for item in myItems:  
            self.weights.append(item[0].encode('gbk'))  
            self.points.append(item[1].encode('gbk'))  
  
              
    # 将内容从页面代码中抠出来  
    def print_data(self,items):    
        for item in items:    
            print item  
              
#调用    
mySpider = SDU_Spider()    
mySpider.sdu_init()    
